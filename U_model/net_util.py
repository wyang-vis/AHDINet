import torch as thimport torch.nn.functional as Ffrom torch import nnfrom .size_adapter import SizeAdapterimport torchfrom .arches import *from einops import rearrangeimport numbersclass ChannelAttention(nn.Module):    def __init__(self, in_planes, ratio=16):        super(ChannelAttention, self).__init__()        self.avg_pool = nn.AdaptiveAvgPool2d(1)        self.max_pool = nn.AdaptiveMaxPool2d(1)        self.fc1 = nn.Conv2d(in_planes, in_planes // 16, 1, bias=False)        self.relu1 = nn.ReLU(inplace=True)        self.fc2 = nn.Conv2d(in_planes // 16, in_planes, 1, bias=False)        self.sigmoid = nn.Sigmoid()    def forward(self, x):        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))        out = avg_out + max_out        out=self.sigmoid(out)        return outclass Weight_Fusion(nn.Module):    def __init__(self, in_planes, ratio=16,L=32,M=2):        super(Weight_Fusion, self).__init__()        self.avg_pool = nn.AdaptiveAvgPool2d(1)        self.max_pool = nn.AdaptiveMaxPool2d(1)        self.in_planes=in_planes        self.M=M        d = max(in_planes // ratio, L)        self.fc1=nn.Sequential(nn.Conv2d(in_planes,d,1,bias=False),                               nn.ReLU(inplace=True))        self.fc2=nn.Conv2d(d,in_planes*2,1,1,bias=False)        self.softmax=nn.Softmax(dim=1)        self.sigmoid = nn.Sigmoid()    def forward(self, x1,x2):        x=x1+x2        avg_out=self.avg_pool(x)        max_out=self.max_pool(x)        out = avg_out + max_out        out = self.fc1(out)        out_two = self.fc2(out)        batch_size = x.size(0)        out_two=out_two.reshape(batch_size,self.M,self.in_planes,-1)        # out_two = self.softmax(out_two)        out_two = self.sigmoid(out_two)        w_1, w_2 = out_two[:, 0:1, :, :], out_two[:, 1:2, :, :]        w_1 = w_1.reshape(batch_size, self.in_planes, 1, 1)        w_2 = w_2.reshape(batch_size, self.in_planes, 1, 1)        out = w_1 * x1 + w_2 * x2        return out# Channel Attention Layerclass CALayer(nn.Module):    def __init__(self, channel, reduction=16, bias=False):        super(CALayer, self).__init__()        # global average pooling: feature --> point        self.avg_pool = nn.AdaptiveAvgPool2d(1)        # feature channel downscale and upscale --> channel weight        self.conv_du = nn.Sequential(                nn.Conv2d(channel, channel // reduction, 1, padding=0, bias=bias),                nn.ReLU(inplace=True),                nn.Conv2d(channel // reduction, channel, 1, padding=0, bias=bias),                nn.Sigmoid()        )    def forward(self, x):        y = self.avg_pool(x)        y = self.conv_du(y)        return x * y## Channel Attention Block (CAB)class CAB(nn.Module):    def __init__(self, in_feat,kernel_size, reduction, bias, act):        super(CAB, self).__init__()        modules_body = []        modules_body.append(conv(in_feat, in_feat, kernel_size, bias=bias))        modules_body.append(act)        modules_body.append(conv(in_feat, in_feat, kernel_size, bias=bias))        self.CA = CALayer(in_feat, reduction, bias=bias)        self.body = nn.Sequential(*modules_body)    def forward(self, x):        x = self.body(x)        res = self.CA(x)        res += x        return resclass EN_Block(nn.Module):    def __init__(self, in_channels, out_channels,BIN,kernel_size=3, reduction=4, bias=False):        super(EN_Block, self).__init__()        self.BIN=BIN        act = nn.ReLU(inplace=True)        self.conv=conv(in_channels, out_channels, 3, bias=bias)        self.CABs = [CAB(out_channels,kernel_size, reduction, bias=bias, act=act) for _ in range(2)]        self.CABs = nn.Sequential(*self.CABs)    def forward(self, x):        x=self.conv(x)        x=self.CABs(x)        return xclass DE_Block(nn.Module):    def __init__(self, in_planes, planes, kernel_size=3, reduction=4, bias=False):        super(DE_Block, self).__init__()        act = nn.ReLU(inplace=True)        self.up=SkipUpSample(in_planes, planes)        self.decoder = [CAB(planes, kernel_size, reduction, bias=bias, act=act) for _ in range(2)]        self.decoder = nn.Sequential(*self.decoder)        self.skip_attn = CAB(planes, kernel_size, reduction, bias=bias, act=act)    def forward(self, x, skpCn):        x = self.up(x, self.skip_attn(skpCn))        x = self.decoder(x)        return xclass BiasFree_LayerNorm(nn.Module):    def __init__(self, normalized_shape):        super(BiasFree_LayerNorm, self).__init__()        if isinstance(normalized_shape, numbers.Integral):            normalized_shape = (normalized_shape,)        normalized_shape = torch.Size(normalized_shape)        assert len(normalized_shape) == 1        self.weight = nn.Parameter(torch.ones(normalized_shape))        self.normalized_shape = normalized_shape    def forward(self, x):        sigma = x.var(-1, keepdim=True, unbiased=False)        return x / torch.sqrt(sigma + 1e-5) * self.weightclass WithBias_LayerNorm(nn.Module):    def __init__(self, normalized_shape):        super(WithBias_LayerNorm, self).__init__()        if isinstance(normalized_shape, numbers.Integral):            normalized_shape = (normalized_shape,)        normalized_shape = torch.Size(normalized_shape)        assert len(normalized_shape) == 1        self.weight = nn.Parameter(torch.ones(normalized_shape))        self.bias = nn.Parameter(torch.zeros(normalized_shape))        self.normalized_shape = normalized_shape    def forward(self, x):        mu = x.mean(-1, keepdim=True)        sigma = x.var(-1, keepdim=True, unbiased=False)        return (x - mu) / torch.sqrt(sigma + 1e-5) * self.weight + self.biasclass LayerNorm(nn.Module):    def __init__(self, dim, LayerNorm_type):        super(LayerNorm, self).__init__()        if LayerNorm_type == 'BiasFree':            self.body = BiasFree_LayerNorm(dim)        else:            self.body = WithBias_LayerNorm(dim)    def forward(self, x):        h, w = x.shape[-2:]        return to_4d(self.body(to_3d(x)), h, w)class Spatio_Attention(nn.Module):    def __init__(self, dim, num_heads, bias):        super(Spatio_Attention, self).__init__()        self.num_heads = num_heads        self.temperature = nn.Parameter(torch.ones(num_heads, 1, 1))        self.q = nn.Conv2d(dim, dim, kernel_size=1, bias=bias)        self.k = nn.Conv2d(dim, dim, kernel_size=1, bias=bias)        self.v = nn.Conv2d(dim, dim, kernel_size=1, bias=bias)        self.project_out = nn.Conv2d(dim, dim, kernel_size=1, bias=bias)    def forward(self, inp):        b, c, h, w = inp.shape        q = self.q(inp)  # image        k = self.k(inp)  # event        v = self.v(inp)  # event        q = rearrange(q, 'b (head c) h w -> b head c (h w)', head=self.num_heads)        k = rearrange(k, 'b (head c) h w -> b head c (h w)', head=self.num_heads)        v = rearrange(v, 'b (head c) h w -> b head c (h w)', head=self.num_heads)        q = torch.nn.functional.normalize(q, dim=-1)        k = torch.nn.functional.normalize(k, dim=-1)        attn = ( k.transpose(-2, -1)@q ) * self.temperature        attn = attn.softmax(dim=-1)        return attn,v